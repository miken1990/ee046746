{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "\n",
    "\n",
    "## Homework 1 - Features Descriptors\n",
    "---\n",
    "\n",
    "### <a style='color:red'> Due Date: 21.4.2020 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "#### READ THIS CAREFULLY\n",
    "* Submission only in **pairs**, on the course website (Moodle). Please refer farther explanation [here](https://moodle.technion.ac.il/mod/groupselect/view.php?id=760560).  \n",
    "\n",
    "\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, locally with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or online on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "\n",
    "* You should submit two **separated** files:\n",
    "    1. A compressed `.zip` file, with the name: `ee046746_hw1_id1_id2.zip`, which contains the followings:\n",
    "        - A folder named `code` with all the code files inside (`.py` or `.ipynb` ONLY!). It is advisable to separate into folders, according to the parts of the exercise (e.g. Part A, Part B).\n",
    "        - A folder named `output` with all the output files you are requested throughout the assignment.\n",
    "        - The code should run on every computer and require no special preparation.\n",
    "    2. A report file with the name `ee046746_hw1_id1_id2.pdf`.\n",
    "        - The report will include a page or two for each exercise. \n",
    "        - The summary should include an explanation of the exercise and how it was run, answers to questions if there were, conclusions and visual results.\n",
    "    \n",
    "> Important Notes:    \n",
    ">* No other file-types (`.docx`, `.html`, ...) will be accepted.\n",
    ">* **No handwritten submissions**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/python.png\" style=\"height:50px;display:inline\"> Python Libraries\n",
    "---\n",
    "\n",
    "* `numpy`\n",
    "* `matplotlib`\n",
    "* `opencv` (or `scikit-image`)\n",
    "* `scikit-learn`\n",
    "* `scipy`\n",
    "* Anything else you need (`os`, `pandas`, `csv`, `json`,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Tasks\n",
    "---\n",
    "* In all tasks, you should document your process and results in a report file (which will be saved as `.pdf`). \n",
    "* You can reference your code in the report file, but no need for actual code in this file, the code is submitted in a seprate folder as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction \n",
    "---\n",
    "In this homework, we will implement an interest point (keypoint) detector similar to SIFT. Then, we will describe the region around each keypoint using a feature descriptor.\n",
    "In class, we have seen the SIFT keypoint extraction and description extraction. In this HW, we will implement the BRIEF, which is another commonly used feature descriptor.\n",
    "The BRIEF is more compact and quicker, which allows real-time computation. Additionally, its performance is powerful just as more complex descriptors like SIFT for many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Keypoint Detector\n",
    "---\n",
    "The first part will include implementing an interest point detector, similar to SIFT. Additional details for the chosen implementation can be found in [2]. \n",
    "In order to find keypoints, we will use the Difference of Gaussian (DoG) detector [1]. We will use a simplified version of (DoG) as described in section 3 of [2]. \n",
    "\n",
    "NOTE: The parameters to use for the following sections are:\n",
    "$$\\sigma_0 = 1, k =\\sqrt 2, levels = [-1; 0; 1; 2; 3; 4], \\theta_c = 0.03 \\text{ and } \\theta_r = 12$$\n",
    "\n",
    "* For the following sections (part 1), add all your functions to one file named `my_keypoint_det.py` or `my_keypoint_det.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Load Image\n",
    "Load the `model_chickenbroth.jpg` image and show it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'code_ex1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-05082e6e9612>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcode_ex1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPartA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmy_keypoint_det\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmkd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'code_ex1'"
     ]
    }
   ],
   "source": [
    "# imports for hw1 (you can add any other library as well)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import cv2\n",
    "\n",
    "import code_ex1.PartA.my_keypoint_det as mkd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "im = cv2.imread('data/model_chickenbroth.jpg')\n",
    "plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "_ = plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Gaussian Pyramid\n",
    "Before we construct a DoG pyramid, we need to construct a Gaussian Pyramid by progressively applying a low pass Gaussian filter to the input image.\n",
    "We provide you the following function `createGaussianPyramid` which gets a grayscale image with values between 0 to 1 (hint: normalize your input image and convert to grayscale). This function outputs GaussianPyramid matrix, which is a set of $L=len(levels)$ blurred images.\n",
    "\n",
    "What is the shape of GaussianPyramid matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def createGaussianPyramid(im, sigma0, k, levels):\n",
    "    GaussianPyramid = []\n",
    "    for i in range(len(levels)):\n",
    "        sigma_ = sigma0 * k ** levels[i]\n",
    "        size = int(np.floor( 3 * sigma_ * 2) + 1)\n",
    "        blur = cv2.GaussianBlur(im,(size,size),sigma_)\n",
    "        GaussianPyramid.append(blur)\n",
    "\n",
    "    res = np.stack(GaussianPyramid)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following function to visualize your pyramid. \n",
    "* Add the results to your PDF report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def displayPyramid(pyramid):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.imshow(np.hstack(pyramid), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short example of using the above functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# example:\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "im = im / 255\n",
    "sigma0 = 1\n",
    "k = np.sqrt(2)\n",
    "levels = [-1, 0, 1, 2, 3, 4]\n",
    "GaussianPyramid =  createGaussianPyramid(im, sigma0, k, levels) \n",
    "displayPyramid(GaussianPyramid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 The DoG Pyramid\n",
    "In this section we will construct the DoG pyramid. Each level of the DoG is constructed by substructing two levels of the Gaussian pyramid:\n",
    "\n",
    "$$D_l(x,y,\\sigma_l) = (G(x,y,\\sigma_{l-1})-G(x,y,\\sigma_l))*I(x,y)$$\n",
    "\n",
    "Where $G(x,y,\\sigma_l)$ is the Gaussian filter used at level $l$ in the Gaussian pyramid, $I(x,y)$ is the original image, and $*$ is the *convolution* operator.\n",
    "\n",
    "We can simplify the eqution due to the distributive property of convolution:\n",
    "\n",
    "$$D_l(x,y,\\sigma_l) = G(x,y,\\sigma_{l-1})*I(x,y)-G(x,y,\\sigma_l)*I(x,y)=GP_l-GP_{l-1}$$\n",
    "\n",
    "Where $GP_l$ is the level $l$ in the Gaussian pyramid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write the following function to constract a DoG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def createDoGPyramid(GaussianPyramid, levels):\n",
    "    # Produces DoG Pyramid\n",
    "    # inputs\n",
    "    # Gaussian Pyramid - A matrix of grayscale images of size\n",
    "    #                    (len(levels), shape(im))\n",
    "    # levels      - the levels of the pyramid where the blur at each level is\n",
    "    #               outputs\n",
    "    # DoG Pyramid - size (len(levels) - 1, shape(im)) matrix of the DoG pyramid\n",
    "    #               created by differencing the Gaussian Pyramid input\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    DoGPyramid, DoGLevels = mkd.createDoGPyramid(GaussianPyramid, levels)\n",
    "    return DoGPyramid, DoGLevels\n",
    "\n",
    "DoGPyramid, DoGLevels = createDoGPyramid(GaussianPyramid, levels)\n",
    "displayPyramid(DoGPyramid)\n",
    "print(DoGLevels)\n",
    "print(DoGPyramid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function should return DoGPyramid an $(L-1)\\times imH \\times imW$ matrix, where $imH\\times imW$ is the original image resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4 Edge Suppression\n",
    "The Difference of Gaussian function responds strongly on corners and edges in addition to blob-like objects. However, edges are not desirable for feature extraction as they are not as distinctive and do not provide a substantially stable localization for keypoints.\n",
    "\n",
    "Here, we will implement the edge removal method described in Section 4.1 of [2], which is based on the principal curvature ratio in a local neighborhood of a point. The paper presents the observation that edge points will have a \"large principal curvature across\n",
    "the edge but a small one in the perpendicular direction.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def computePrincipalCurvature(DoGPyramid):\n",
    "    # Edge Suppression\n",
    "    #  Takes in DoGPyramid generated in createDoGPyramid and returns\n",
    "    #  PrincipalCurvature,a matrix of the same size where each point contains the\n",
    "    #  curvature ratio R for the corre-sponding point in the DoG pyramid\n",
    "    # \n",
    "    #  INPUTS\n",
    "    #  DoG Pyramid - size (len(levels) - 1, shape(im)) matrix of the DoG pyramid\n",
    "    # \n",
    "    #  OUTPUTS\n",
    "    #  PrincipalCurvature - size (len(levels) - 1, shape(im)) matrix where each \n",
    "    #                       point contains the curvature ratio R for the \n",
    "    #                       corresponding point in the DoG pyramid\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    PrincipalCurvature = mkd.computePrincipalCurvature(DoGPyramid)\n",
    "    return PrincipalCurvature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes in DoGPyramid generated in the previous section and returns PrincipalCurvature, a matrix of the same size where each point contains the curvature ratio R for the corresponding point in the DoG pyramid:\n",
    "\n",
    "$ R = \\frac{TR(H)^2}{Det(H)} = \\frac{(\\lambda_{min}+\\lambda_{max})^2}{\\lambda_{min}\\lambda_{max}}$\n",
    "\n",
    "where H is the Hessian of the Difference of Gaussian function (i.e. one level of the DoG pyramid) computed by using pixel differences as mentioned in Section 4.1 of [2]. **Use the Sobel filter to  compute the second order derivatives** (hint: cv2.Sobel()).\n",
    "\n",
    "$ H = \\begin{bmatrix}\n",
    "D_{xx} & D_{xy}\\\\\n",
    "D_{yx} & D_{yy}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is similar in spirit to but different than the Harris corner detection matrix you saw in class. Both methods examine the eigenvalues $\\lambda$ of a matrix, but the method in [2] performs a test without requiring the direct computation of the eigenvalues. Note that you need to compute each term of the Hessian before being able to take the trace and\n",
    "determinant.\n",
    "\n",
    "We can see that R reaches its minimum when the two eigenvalues $\\lambda_{min}$ and $\\lambda_{max}$ are equal, meaning that the curvature is the same in the two principal directions. Edge points, in general, will have a principal curva\n",
    "ture significantly larger in one direction\n",
    "than the other. To remove edge points, we simply check against a threshold R > $\\theta_r$.\n",
    "<!-- Fig. 3 shows t\n",
    "he DoG detector with and without edge suppression. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5 Detecting Extrema\n",
    "To detect corner-like, scale-invariant interest points, the DoG detector chooses points that are local extrema in both scale and space. Here, we will consider a point’s eight neighbors in space and its two neighbors in scale (one in the scale above and one in the scale below).\n",
    "* write the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def getLocalExtrema(DoGPyramid, DoGLevels, PrincipalCurvature, \n",
    "                    th_contrast, th_r): \n",
    "    #     Returns local extrema points in both scale and space using the DoGPyramid\n",
    "    #     INPUTS\n",
    "    #         DoG_pyramid - size (len(levels) - 1, imH, imW ) matrix of the DoG pyramid\n",
    "    #         DoG_levels  - The levels of the pyramid where the blur at each level is\n",
    "    #                       outputs\n",
    "    #         principal_curvature - size (len(levels) - 1, imH, imW) matrix contains the\n",
    "    #                       curvature ratio R\n",
    "    #         th_contrast - remove any point that is a local extremum but does not have a\n",
    "    #                       DoG response magnitude above this threshold\n",
    "    #         th_r        - remove any edge-like points that have too large a principal\n",
    "    #                       curvature ratio\n",
    "    #      OUTPUTS\n",
    "    #         locsDoG - N x 3 matrix where the DoG pyramid achieves a local extrema in both\n",
    "    #                scale and space, and also satisfies the two thresholds.\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    locsDoG = mkd.getLocalExtrema(DoGPyramid, DoGLevels, PrincipalCurvature, th_contrast, th_r)\n",
    "    return locsDoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as input `DoGPyramid` and `DoGLevels` from Section 1.3 and `PrincipalCurvature` from Section 1.4. It also takes two threshold values, `th_contrast` and `th_r`. The threshold $\\theta_c$ should remove any point that is a local extremum but does not have a Difference\n",
    "of Gaussian (DoG) response magnitude above this threshold (i.e. $|D(x, y, \\sigma)| > \\theta_c$). The threshold $\\theta_r$ should remove any edge-like points that have too large a principal curvature ratio specified by `PrincipalCurvature`.\n",
    "\n",
    "The function should return `locsDoG`, a $N \\times 3$  ($N$ is the number of the detected extrema points) matrix where the DoG pyramid achieves a local extrema in both scale and space, and also satisfies the two thresholds. The first and second column of `locsDoG` should be the $(x, y)$ values of the local extremum and the third column should contain the corresponding level of the DoG pyramid where it was detected (try to eliminate loops in the function so that it runs efficiently).\n",
    "\n",
    "NOTE: In all implementations, we assume the $x$ coordinate corresponds to\n",
    "columns and $y$ coordinate corresponds to rows. For example, the coordinate\n",
    "$(10, 20)$ corresponds to the (row 20, column 10) in the image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6 Putting it Together\n",
    "* Write the following function to combine the above parts into a DoG detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def DoGdetector(im, sigma0, k, levels,\n",
    "    th_contrast, th_r):\n",
    "    #     Putting it all together\n",
    "    #     Inputs          Description\n",
    "    #     --------------------------------------------------------------------------\n",
    "    #     im              Grayscale image with range [0,1].\n",
    "    #     sigma0          Scale of the 0th image pyramid.\n",
    "    #     k               Pyramid Factor.  Suggest sqrt(2).\n",
    "    #     levels          Levels of pyramid to construct. Suggest -1:4.\n",
    "    #     th_contrast     DoG contrast threshold.  Suggest 0.03.\n",
    "    #     th_r            Principal Ratio threshold.  Suggest 12.\n",
    "    #     Outputs         Description\n",
    "    #     --------------------------------------------------------------------------\n",
    "    #     locsDoG         N x 3 matrix where the DoG pyramid achieves a local extrema\n",
    "    #                     in both scale and space, and satisfies the two thresholds.\n",
    "    #     gauss_pyramid   A matrix of grayscale images of size (len(levels),imH,imW)\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    locsDoG, GaussianPyramid = mkd.DoGdetector(im, sigma0, k, levels, th_contrast, th_r)\n",
    "    return locsDoG, GaussianPyramid\n",
    "\n",
    "# prepare image \n",
    "im = cv2.imread('data/pf_scan_scaled.jpg')\n",
    "im_plot = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "im_gray = im_gray / 255\n",
    "\n",
    "locsDoG, _ = DoGdetector(im_gray, sigma0, k, levels, th_contrast=0.03, th_r=12)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(im_plot)\n",
    "\n",
    "for arr in locsDoG:\n",
    "    circ  = Circle((arr[0],arr[1]), arr[2] + 1, fill=False, color='red')\n",
    "    ax.add_patch(circ)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should take in a grayscale image, `im`, scaled between 0 and 1, and the parameters `sigma0, k, levels, th_contrast`, and `th_r`. It should use each of the above functions and return the keypoints in `locsDoG` and the Gaussian pyramid in `GaussianPyramid`.\n",
    "Note that we are dealing with real images here, so your keypoint detector may find points with high scores that you do not perceive to be corners.\n",
    "\n",
    "* Include the image with the detected keypoints in your PDF report. You can use any of the provided images.\n",
    "* Take a step outside (if the government allows) take a picture, and apply your keypoints detector. Do you get reasonable results? How can you improve the results? Add the result and discussion to your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - BRIEF Descriptor\n",
    "---\n",
    "Now that we have interest points that tell us where to find the most informative feature points in the image, we would like to describe each keypoint region with a descriptor. Then we can use those descriptors to match corrasponding points between different images. The BRIEF descriptor encodes information from a 9 × 9 patch $p$ centered around the interest point at the characteristic scale of the\n",
    "interest point. You can read more in\n",
    "<a href=\"https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_brief/py_brief.html\">BRIEF</a>.\n",
    "\n",
    "* For the following sections (part 2), add all your functions to one file named `my_BRIEF.py` or `my_BRIEF.ipynb`.\n",
    "\n",
    "\n",
    "#### 2.1 Creating a Set of BRIEF Tests\n",
    "The descriptor itself is a vector that is $n$-bits long, where each bit is the result of the following simple test:\n",
    "\n",
    "$ \\rho(p;x,y):= \\begin{cases}\n",
    "1, & \\text{ if } p(x)<p(y) \\\\ \n",
    " 0, & \\text{ otherwise. } \\\\\n",
    "\\end{cases}\\\\\n",
    "\\\\\n",
    "x,y \\in N^{S^2}\\\\\n",
    "p \\in R^{S^2}\n",
    "$\n",
    "\n",
    "Where $S=9$ is the width and hight sizes of a patch $p$, so $x,y$ are each a pixel location within a flatten patch.\n",
    "Set $n$ to 256 bits. There is no need to encode the test results as actual bits. It is fine to encode them as a 256 element vector. \n",
    "\n",
    "There are many choices for the 256 test pairs $(x,y)$ used to compute $\\rho (p; x,y)$ (each of the $n$ bits). The authors describe and test some of them in <a href=\"https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Images/team_lepetit/publications/calonder_eccv10.pdf\">[3]</a>. Read section 3.2 of that paper and implement one of these solutions. You should generate a static set of test pairs and save that data to a file. You will use these pairs for all subsequent computations of the BRIEF descriptor.\n",
    "\n",
    "* Write the function to create the $x$ and $y$ pairs that we will use for comparison to compute $\\rho$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat, loadmat\n",
    "import code_ex1.PartB.my_BRIEF as brief\n",
    "def makeTestPattern(patchWidth, nbits):\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    compareX, compareY = brief.makeTestPattern(patchWidth, nbits)\n",
    "    return compareX, compareY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`patchWidth` is the width of the image patch (usually 9) and `nbits` is the number of tests $n$ in the BRIEF descriptor. `compareX` and `compareY` are linear indices into the `patchWidth` $\\times$ `patchWidth` image patch and are each `nbits` $\\times$ 1 vectors. Run this routine for the given parameters `patchWidth = 9` and `n = 256` and save the results in `testPattern.mat`. You can use `scipy.io.savemat()`. <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.savemat.html#scipy.io.savemat\">Read more here</a>.\n",
    "\n",
    "* Include this file in your submission (`code` directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  Compute the BRIEF Descriptor\n",
    "Now we can compute the BRIEF descriptor for the detected keypoints.\n",
    "* Write the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "def computeBrief(im, GaussianPyramid, locsDoG, k, levels,\n",
    "compareX, compareY):\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    locs,desc = brief.computeBrief(im, GaussianPyramid, locsDoG, k, levels,\n",
    "                        compareX, compareY)\n",
    "    return locs,desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `im` is a grayscale image with values from 0 to 1, `locsDoG` are the keypoint locations returned by the DoG detector from Section 1.6, `levels` are the Gaussian scale levels that were given in Section 1, and `compareX` and `compareY` are the test patterns computed in Section 2.1 and were saved into `testPattern.mat` (load them with `scipy.io.loadmat()`, <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html\">read more</a>).\n",
    "\n",
    "The function returns `locs`, an $m \\times 3$ vector, where the first two columns are the image coordinates of keypoints and the third column is the pyramid level of the keypoints, and desc is an $m \\times n$ bits matrix of stacked BRIEF descriptors. `m` is the number of valid descriptors in the image and will vary. You may have to be careful about the input DoG detector locations since they may be at the edge of an image where we cannot extract a full patch of width `patchWidth`. Thus, the number of output locs may be less than the input `locsDoG`. Note: Its possible that you may not require all the arguments to this function to compute the desired output. They have just been provided to permit the use of any of some different approaches to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3  Putting it all Together\n",
    "* Write a function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def briefLite(im):\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    locs, desc = brief.briefLite(im)\n",
    "    return locs, desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which accepts a grayscale image `im` with values between 0 and 1 and returns `locs`, an $m \\times 3$ vector, where the first two columns are the image coordinates of keypoints and the third column is the pyramid level of the keypoints, and `desc`, an $ m \\times n$ bits matrix of stacked BRIEF descriptors. `m` is the number of valid descriptors in the image and\n",
    "will vary. `n` is the number of bits for the BRIEF descriptor.\n",
    "\n",
    "This function should perform all the necessary steps to extract the descriptors from the image, including: (1) Load parameters and test patterns, (2) Get keypoint locations, and (3) Compute a set of valid BRIEF descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4   Check Point: Descriptor Matching\n",
    "A descriptor’s strength is in its ability to match to other descriptors generated by the same world point despite change of view, lighting, etc. The distance metric used to compute the similarity between two descriptors is critical. For BRIEF, this distance metric is the Hamming distance. The Hamming distance is simply the number of bits in two descriptors that differ. (Note that the position of the bits matters.)\n",
    "\n",
    "To perform the descriptor matching mentioned above, we have provided you the function `briefMatch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def briefMatch(desc1, desc2, ratio=0.5):   \n",
    "    #     performs the descriptor matching\n",
    "    #     inputs  : desc1 , desc2 - m1 x n and m2 x n matrix. m1 and m2 are the number of keypoints in image 1 and 2.\n",
    "    #                                 n is the number of bits in the brief\n",
    "    #     outputs : matches - p x 2 matrix. where the first column are indices\n",
    "    #                                         into desc1 and the second column are indices into desc2  \n",
    "    matches = brief.briefMatch(desc1, desc2, ratio=0.5)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which accepts an $m1 \\times n$ bits stack of BRIEF descriptors from a first image and a $m2 \\times n$ bits stack of BRIEF descriptors from a second image and returns a $p \\times 2$ matrix of matches, where the first column are indices into `desc1` and the second column are indices into `desc2`. Note that `m1`, `m2`, and `p` may be different sizes and $p \\leq \\min(m1, m2)$.\n",
    "\n",
    "* Write a test script testMatch to load two of the chickenbroth images and compute feature matches. Use the provided `plotMatches` and `briefMatch` functions to visualize the result.\n",
    "\n",
    "* Use the following function to display the matched points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMatches(im1, im2, matches, locs1, locs2):\n",
    "    fig = plt.figure()\n",
    "    # draw two images side by side\n",
    "    imH = max(im1.shape[0], im2.shape[0])\n",
    "    im = np.zeros((imH, im1.shape[1]+im2.shape[1]), dtype='uint8')\n",
    "    im[0:im1.shape[0], 0:im1.shape[1]] = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im[0:im2.shape[0], im1.shape[1]:] = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    for i in range(matches.shape[0]):\n",
    "        pt1 = locs1[matches[i,0], 0:2]\n",
    "        pt2 = locs2[matches[i,1], 0:2].copy()\n",
    "        pt2[0] += im1.shape[1]\n",
    "        x = np.asarray([pt1[0], pt2[0]])\n",
    "        y = np.asarray([pt1[1], pt2[1]])\n",
    "        plt.plot(x,y,'r')\n",
    "        plt.plot(x,y,'g.')\n",
    "    plt.show()\n",
    "    \n",
    "im1 = cv2.imread('data/pf_scan_scaled.jpg')\n",
    "im1_gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "im1_gray = im1_gray / 255\n",
    "\n",
    "im2 = cv2.imread('data/pf_stand.jpg')\n",
    "im2_gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "im2_gray = im2_gray / 255\n",
    "# im = cv2.imread('data/model_chickenbroth.jpg')\n",
    "# plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "locs1, desc1 = brief.briefLite(im1_gray)\n",
    "locs2, desc2 = brief.briefLite(im2_gray)\n",
    "matches = brief.briefMatch(desc1, desc2)\n",
    "plotMatches(im1, im2, matches, locs1, locs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `im1` and `im2` are grayscale images from 0 to 1, `matches` is the list of matches returned by `briefMatch` and `locs1` and `locs2` are the locations of keypoints from `briefLite`.\n",
    "\n",
    "* Save the resulting figure and submit it in your PDF report. Also, present results with the two `incline*.jpg` images and with the computer vision textbook cover page (template is in file `pf_scan_scaled.jpg`) against the other `pf_*` images. Briefly discuss any cases that perform worse or better.\n",
    "\n",
    "* Suggestion for debugging: A good test of your code is to check that you can match an image to itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5   BRIEF and rotations (Bonus)\n",
    "\n",
    "You may have noticed worse performance under rotations. Let’s investigate this!\n",
    "\n",
    "* Take the `model_chickenbroth.jpg` test image and match it to itself while rotating the second image (hint: <a href=\"https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html\">openCV_rotate</a>) in increments of 10 degrees. Count the number of correct matches at each rotation and construct a bar graph showing rotation angle vs the number of correct matches. Include this in your PDF and explain why you think the descriptor behaves this way. Create a script `briefRotTest.py` that performs this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from code_ex1.PartB.briefRotTest import get_rotations_stats_bar_graph\n",
    "path = 'data/model_chickenbroth.jpg'\n",
    "get_rotations_stats_bar_graph(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> References & Credits\n",
    "* [1] P. Burt and E. Adelson. The Laplacian Pyramid as a Compact Image Code. IEEE\n",
    "Transactions on Communications, 31(4):532{540, April 1983.\n",
    "* [2] David G. Lowe. Distinctive Image Features from Scale-Invariant Keypoints. Inter-\n",
    "national Journal of Computer Vision, 60(2):91{110, November 2004.\n",
    "* [3] Michael Calonder, Vincent Lepetit, Christoph Strecha, and Pascal Fua. BRIEF :\n",
    "Binary Robust Independent Elementary Features.\n",
    "* Carnegie Mellon University - CMU\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
